{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piper Finetuning\n",
    "\n",
    "This is a nb to finetune piper voices\n",
    "\n",
    "# Sources\n",
    "- https://www.youtube.com/watch?v=b_we_jma220\n",
    "- https://github.com/rhasspy/piper/blob/master/TRAINING.md\n",
    "\n",
    "\n",
    "# Training Guide\n",
    "\n",
    "Check out a [video training guide by Thorsten MÃ¼ller](https://www.youtube.com/watch?v=b_we_jma220)\n",
    "\n",
    "For Windows, see [ssamjh's guide using WSL](https://ssamjh.nz/create-custom-piper-tts-voice/)\n",
    "\n",
    "---\n",
    "\n",
    "Training a voice for Piper involves 3 main steps:\n",
    "\n",
    "1. Preparing the dataset\n",
    "2. Training the voice model\n",
    "3. Exporting the voice model\n",
    "\n",
    "Choices must be made at each step, including:\n",
    "\n",
    "* The model \"quality\"\n",
    "    * low = 16,000 Hz sample rate, [smaller voice model](https://github.com/rhasspy/piper/blob/master/src/python/piper_train/vits/config.py#L30)\n",
    "    * medium = 22,050 Hz sample rate, [smaller voice model](https://github.com/rhasspy/piper/blob/master/src/python/piper_train/vits/config.py#L30)\n",
    "    * high = 22,050 Hz sample rate, [larger voice model](https://github.com/rhasspy/piper/blob/master/src/python/piper_train/vits/config.py#L45)\n",
    "* Single or multiple speakers\n",
    "* Fine-tuning an [existing model](https://huggingface.co/datasets/rhasspy/piper-checkpoints/tree/main) or training from scratch\n",
    "* Exporting to [onnx](https://github.com/microsoft/onnxruntime/) or PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting Started\n",
    "\n",
    "Start by installing system dependencies:\n",
    "\n",
    "``` sh\n",
    "sudo apt-get install python3-dev\n",
    "```\n",
    "\n",
    "Then create a Python virtual environment:\n",
    "\n",
    "``` sh\n",
    "cd piper/src/python\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "pip3 install --upgrade pip\n",
    "pip3 install --upgrade wheel setuptools\n",
    "pip3 install -e .\n",
    "```\n",
    "\n",
    "Run the `build_monotonic_align.sh` script in the `src/python` directory to build the extension.\n",
    "\n",
    "Ensure you have [espeak-ng](https://github.com/espeak-ng/espeak-ng/) installed (`sudo apt-get install espeak-ng`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "!cd piper/src/python\n",
    "!python3 -m venv .venv\n",
    "!source .venv/bin/activate\n",
    "# !pip3 install --upgrade pip\n",
    "!pip3 install pip==21.0.1\n",
    "!pip3 install --upgrade wheel setuptools\n",
    "!pip3 install --upgrade pandas\n",
    "if (not os.path.exists('piper')):\n",
    "    !git clone https://github.com/rhasspy/piper.git\n",
    "!pip3 install -r piper/src/python/requirements.txt\n",
    "!pip3 install -e piper/src/python\n",
    "!pip3 install torchaudio==0.11.0 torchmetrics==0.11.4\n",
    "# !pip3 install numpy==1.20\n",
    "!pip3 install --force-reinstall numpy==1.26.4\n",
    "# https://stackoverflow.com/a/75702229/6559381\n",
    "!pip3 install --force-reinstall torch==1.12.1+cu113 --extra-index-url https://download.pytorch.org/whl/\n",
    "!pip3 list | grep numpy\n",
    "!pip3 list | grep piper\n",
    "!pip3 list | grep torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the build_monotonic_align.sh script in the src/python directory to build the extension.\n",
    "# !git clone https://github.com/rhasspy/piper.git\n",
    "!chmod +x piper/src/python/build_monotonic_align.sh\n",
    "!./piper/src/python/build_monotonic_align.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "voice_name = 'jarvis'\n",
    "training_path = os.path.join(os.getcwd(), 'content/dataset/' + voice_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a dataset\n",
    "\n",
    "The Piper training scripts expect two files that can be generated by `python3 -m piper_train.preprocess`:\n",
    "\n",
    "* A `config.json` file with the voice settings\n",
    "    * `audio` (required)\n",
    "        * `sample_rate` - audio rate in hertz\n",
    "    * `espeak` (required)\n",
    "        * `language` - espeak-ng voice or [alphabet](https://github.com/rhasspy/piper-phonemize/blob/master/src/phoneme_ids.cpp)\n",
    "    * `num_symbols` (required)\n",
    "        * Number of phonemes in the model (typically 256)\n",
    "    * `num_speakers` (required)\n",
    "        * Number of speakers in the dataset\n",
    "    * `phoneme_id_map` (required)\n",
    "        * Map from a phoneme (UTF-8 codepoint) to a list of ids\n",
    "        * Id 0 (\"_\") is padding (pad)\n",
    "        * Id 1 (\"^\") is the beginning of an utterance (bos)\n",
    "        * Id 2 (\"$\") is the end of an utterance (eos)\n",
    "        * Id 3 (\" \") is a word separator (whitespace)\n",
    "    * `phoneme_type`\n",
    "        * \"espeak\" or \"text\"\n",
    "        * \"espeak\" phonemes use [espeak-ng](https://github.com/rhasspy/espeak-ng)\n",
    "        * \"text\" phonemes use a pre-defined [alphabet](https://github.com/rhasspy/piper-phonemize/blob/master/src/phoneme_ids.cpp)\n",
    "    * `speaker_id_map`\n",
    "        * Map from a speaker name to id\n",
    "    * `phoneme_map`\n",
    "        * Map from a phoneme (UTF-8 codepoint) to a list of phonemes\n",
    "    * `inference`\n",
    "        * `noise_scale` - noise added to the generator (default: 0.667)\n",
    "        * `length_scale` - speaking speed (default: 1.0)\n",
    "        * `noise_w` - phoneme width variation (default: 0.8) \n",
    "* A `dataset.jsonl` file with one line per utterance (JSON objects)\n",
    "    * `phoneme_ids` (required)\n",
    "        * List of ids for each utterance phoneme (0 <= id < `num_symbols`)\n",
    "    * `audio_norm_path` (required)\n",
    "        * Absolute path to [normalized audio](https://github.com/rhasspy/piper/tree/master/src/python/piper_train/norm_audio) file (`.pt`)\n",
    "    * `audio_spec_path` (required)\n",
    "        * Absolute path to [audio spectrogram](https://github.com/rhasspy/piper/blob/fda64e7a5104810a24eb102b880fc5c2ac596a38/src/python/piper_train/vits/mel_processing.py#L40) file (`.pt`)\n",
    "    * `speaker_id` (required for multi-speaker)\n",
    "        * Id of the utterance's speaker (0 <= id < `num_speakers`)\n",
    "    * `audio_path`\n",
    "        * Absolute path to original audio file\n",
    "    * `text`\n",
    "        * Original text of utterance before phonemization\n",
    "    * `phonemes`\n",
    "        * Phonemes from utterance text before converting to ids\n",
    "    * `speaker`\n",
    "        * Name of utterance speaker (from `speaker_id_map`)\n",
    "\n",
    "\n",
    "### Dataset Format\n",
    "\n",
    "The pre-processing script expects data to be a directory with:\n",
    "\n",
    "* `metadata.csv` - CSV file with text, audio filenames, and speaker names\n",
    "* `wav/` - directory with audio files\n",
    "\n",
    "The `metadata.csv` file uses `|` as a delimiter, and has 2 or 3 columns depending on if the dataset has a single or multiple speakers.\n",
    "There is no header row.\n",
    "\n",
    "For single speaker datasets:\n",
    "\n",
    "```csv\n",
    "id|text\n",
    "```\n",
    "\n",
    "where `id` is the name of the WAV file in the `wav` directory. For example, an `id` of `1234` means that `wav/1234.wav` should exist. \n",
    "\n",
    "For multi-speaker datasets:\n",
    "\n",
    "```csv\n",
    "id|speaker|text\n",
    "```\n",
    "\n",
    "where `speaker` is the name of the utterance's speaker. Speaker ids will automatically be assigned based on the number of utterances per speaker (speaker id 0 has the most utterances).\n",
    "\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "An example of pre-processing a single speaker dataset:\n",
    "\n",
    "``` sh\n",
    "python3 -m piper_train.preprocess \\\n",
    "  --language en-us \\\n",
    "  --input-dir /path/to/dataset_dir/ \\\n",
    "  --output-dir /path/to/training_dir/ \\\n",
    "  --dataset-format ljspeech \\\n",
    "  --single-speaker \\\n",
    "  --sample-rate 22050\n",
    "```\n",
    "\n",
    "The `--language` argument refers to an [espeak-ng voice](https://github.com/espeak-ng/espeak-ng/) by default, such as `de` for German.\n",
    "\n",
    "To pre-process a multi-speaker dataset, remove the `--single-speaker` flag and ensure that your dataset has the 3 columns: `id|speaker|text`\n",
    "Verify the number of speakers in the generated `config.json` file before proceeding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show metadata.csv\n",
    "import pandas as pd\n",
    "\n",
    "metadata = pd.read_csv(os.path.join(training_path, 'metadata.csv'))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the .ckpt file for the voice that'll be used to finetune upon.\n",
    "# alan: https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_GB/alan/medium/epoch%3D6339-step%3D1647790.ckpt?download=true\n",
    "\n",
    "if not os.path.exists(f'{training_path}/{voice_name}.ckpt'):\n",
    "    !wget -O {training_path}/{voice_name}.ckpt https://huggingface.co/datasets/rhasspy/piper-checkpoints/resolve/main/en/en_GB/alan/medium/epoch%3D6339-step%3D1647790.ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo {training_path}\n",
    "!python3 -m piper_train.preprocess \\\n",
    "  --language en-US \\\n",
    "  --input-dir {training_path} \\\n",
    "  --output-dir {training_path}/output \\\n",
    "  --dataset-format ljspeech \\\n",
    "  --single-speaker \\\n",
    "  --sample-rate 44100 \\\n",
    "  --max-workers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model\n",
    "\n",
    "Once you have a `config.json`, `dataset.jsonl`, and audio files (`.pt`) from pre-processing, you can begin the training process with `python3 -m piper_train`\n",
    "\n",
    "For most cases, you should fine-tune from [an existing model](https://huggingface.co/datasets/rhasspy/piper-checkpoints/tree/main). The model must have the sample audio quality and sample rate, but does not necessarily need to be in the same language.\n",
    "\n",
    "It is **highly recommended** to train with the following `Dockerfile`:\n",
    "\n",
    "``` dockerfile\n",
    "FROM nvcr.io/nvidia/pytorch:22.03-py3\n",
    "\n",
    "RUN pip3 install \\\n",
    "    'pytorch-lightning'\n",
    "\n",
    "ENV NUMBA_CACHE_DIR=.numba_cache\n",
    "```\n",
    "\n",
    "As an example, we will fine-tune the [medium quality lessac voice](https://huggingface.co/datasets/rhasspy/piper-checkpoints/tree/main/en/en_US/lessac/medium). Download the `.ckpt` file and run the following command in your training environment:\n",
    "\n",
    "``` sh\n",
    "python3 -m piper_train \\\n",
    "    --dataset-dir /path/to/training_dir/ \\\n",
    "    --accelerator 'gpu' \\\n",
    "    --devices 1 \\\n",
    "    --batch-size 32 \\\n",
    "    --validation-split 0.0 \\\n",
    "    --num-test-examples 0 \\\n",
    "    --max_epochs 10000 \\\n",
    "    --resume_from_checkpoint /path/to/lessac/epoch=2164-step=1355540.ckpt \\\n",
    "    --checkpoint-epochs 1 \\\n",
    "    --precision 32\n",
    "```\n",
    "\n",
    "Use `--quality high` to train a [larger voice model](https://github.com/rhasspy/piper/blob/master/src/python/piper_train/vits/config.py#L45) (sounds better, but is much slower).\n",
    "\n",
    "You can adjust the validation split (5% = 0.05) and number of test examples for your specific dataset. For fine-tuning, they are often set to 0 because the target dataset is very small.\n",
    "\n",
    "Batch size can be tricky to get right. It depends on the size of your GPU's vRAM, the model's quality/size, and the length of the longest sentence in your dataset. The `--max-phoneme-ids <N>` argument to `piper_train` will drop sentences that have more than `N` phoneme ids. In practice, using `--batch-size 32` and `--max-phoneme-ids 400` will work for 24 GB of vRAM (RTX 3090/4090).\n",
    "\n",
    "\n",
    "### Multi-Speaker Fine-Tuning\n",
    "\n",
    "If you're training a multi-speaker model, use `--resume_from_single_speaker_checkpoint` instead of `--resume_from_checkpoint`. This will be *much* faster than training your multi-speaker model from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "isAvailable = torch.cuda.is_available()\n",
    "\n",
    "if isAvailable:\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    !python3 -m piper_train \\\n",
    "        --dataset-dir {training_path}/output \\\n",
    "        --accelerator 'gpu' \\\n",
    "        --devices 1 \\\n",
    "        --batch-size 32 \\\n",
    "        --validation-split 0.1 \\\n",
    "        --num-test-examples 0 \\\n",
    "        --max_epochs 8000 \\\n",
    "        --resume_from_checkpoint {training_path}/{voice_name}.ckpt \\\n",
    "        --checkpoint-epochs 1 \\\n",
    "        --precision 16\n",
    "else:\n",
    "    !python3 -m piper_train \\\n",
    "        --dataset-dir {training_path}/output \\\n",
    "        --batch-size 32 \\\n",
    "        --validation-split 0.0 \\\n",
    "        --num-test-examples 0 \\\n",
    "        --max_epochs 10000 \\\n",
    "        --resume_from_checkpoint {training_path}/{voice_name}.ckpt \\\n",
    "        --checkpoint-epochs 1 \\\n",
    "        --precision 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_training_version = 2\n",
    "!cat piper/etc/test_sentences/test_en-us.jsonl | \\\n",
    "    python3 -m piper_train.infer \\\n",
    "        --sample-rate 44100 \\\n",
    "        --checkpoint {training_path}/output/lightning_logs/version_{current_training_version}/checkpoints/*.ckpt \\\n",
    "        --output-dir {training_path}/output/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir {training_path}/output/lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a Model\n",
    "\n",
    "When your model is finished training, export it to onnx with:\n",
    "\n",
    "```sh\n",
    "python3 -m piper_train.export_onnx \\\n",
    "    /path/to/model.ckpt \\\n",
    "    /path/to/model.onnx\n",
    "    \n",
    "cp /path/to/training_dir/config.json \\\n",
    "   /path/to/model.onnx.json\n",
    "```\n",
    "\n",
    "The [export script](https://github.com/rhasspy/piper-samples/blob/master/_script/export.sh) does additional optimization of the model with [onnx-simplifier](https://github.com/daquexian/onnx-simplifier).\n",
    "\n",
    "If the export is successful, you can now use your voice with Piper:\n",
    "\n",
    "```sh\n",
    "echo 'This is a test.' | \\\n",
    "  piper -m /path/to/model.onnx --output_file test.wav\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m piper_train.export_onnx \\\n",
    "    {training_path}/output/version_{current_training_version}/checkpoints/model.ckpt \\\n",
    "    {training_path}/output/{voice_name}.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
